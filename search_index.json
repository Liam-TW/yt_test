[["index.html", "Youtube Influencers Trending Model 1 About this project 1.1 研究背景與目的 1.2 研究方法 1.3 研究結果與討論 1.4 結論與實務建議 1.5 參考文獻", " Youtube Influencers Trending Model Liam Chen 2024-11-22 1 About this project 1.1 研究背景與目的 隨著社群媒體的普及，YouTube 已成為全球最大的影片分享平台之一，其不僅為個人創作者提供了展示才華的機會，也為企業提供了宣傳產品、服務及品牌形象的廣大舞台。透過影片的快速傳播與內容的多樣性，YouTube 上的「爆紅影片」現象不斷出現，這類影片往往能夠在短時間內獲得大量的點擊率與觀眾關注，為創作者或相關品牌帶來巨大的曝光和經濟效益。因此，對企業而言，能夠理解並應用這些「爆紅」的關鍵因素，將是優化行銷策略、提升效益的重要途徑。 本研究旨在通過分析 YouTube 上爆紅影片的各項變項與特徵，探討其是否有一定的模式或法則可供參考。藉由此分析，本文期望能為企業在設計產品、服務、或品牌形象宣傳影片時，提供實務上的建議，特別是在如何有效利用外包與內部資源，以降低宣傳成本並提升行銷效益的問題上。本研究將進一步探討，哪些類型或性質的影片適合與網紅合作進行外包製作，又有哪些形式或概念的影片可以透過培訓內部的公關或行銷團隊來完成，以達到成本控制與效果最大化的雙重目標。 1.2 研究方法 本研究將以量化方法為主，對 YouTube 上爆紅影片進行變項分析，主要包括以下步驟： 1.2.1 資料收集 運用 YouTube API 蒐集過去一年內在 YouTube 平台上獲得高點擊率的爆紅影片。收集的變項將包括影片的標題、描述、長度、發布時間、觀看次數、喜好數、留言數、影片類別、以及是否與網紅合作等。 1.2.2 變項選擇 選定影響影片爆紅的關鍵變項，包括影片長度、內容主題（如娛樂、教學、產品展示等）、視覺效果、影片的互動性（如留言數與按讚數）、發布頻率。 Y(Views) = X1(tf-idf) + X2(LDA) + X3(likes) + (timestamp) 1.2.3 數據分析 運用文字探勘、統計分析技術，包括迴歸分析，以及Machine Learning機器學習等，來檢視不同變項與影片「爆紅」之間的關聯性，探討是否存在可預測的模式或法則。 1.2.4 個案分析 針對成功爆紅的影片進行深入的個案分析，解析其成功因素與創作策略，並與不爆紅的影片進行對比，以識別出影片能爆紅的獨特特徵。 1.3 研究結果與討論 透過本研究的數據分析，我們預期將能識別出一些與影片爆紅高度相關的特徵變項，如影片的長度、內容主題的選擇、與網紅的合作模式等。初步假設是，不同類型的影片適合不同的製作與推廣策略，這與影片的內容特質、目標受眾及行銷目標密切相關。 1.3.1 與網紅合作的影片類型 經分析，我們預期某些具有高度娛樂性、涉及流行文化或針對年輕受眾的影片，與網紅合作效果較好，這類影片依賴網紅的個人影響力與社群粉絲基礎，能夠快速獲得大量曝光。因此，建議企業針對這類影片選擇外包模式，與具有相關影響力的網紅合作，達到品牌迅速推廣的效果。 1.3.2 內部團隊製作的影片類型 另一方面，對於企業形象宣傳、產品說明、或具較高專業性的內容，這類影片可以通過培訓內部的公關或行銷團隊來製作。內部團隊對公司價值與產品知識的理解更為深入，因此在精準傳達品牌信息、控制內容品質方面具有優勢。透過內部製作，企業可以大幅降低外包成本，同時確保影片質量與品牌一致性。 1.4 結論與實務建議 本研究發現，YouTube 爆紅影片的形成具有一定的規律性，而不同類型的影片適合不同的製作與推廣方式。針對娛樂性高、針對年輕受眾的影片，企業應考慮與網紅合作，通過外包模式提升影片的影響力。而針對企業形象宣傳、產品說明等具有較高專業性和一致性要求的影片，內部團隊的製作將更為合適。 基於這些結果，我們對企業提出以下建議： 1. 選擇外包合作對象：對於適合外包的影片，應根據網紅的影響力、粉絲群特徵進行合作對象的篩選，以確保影片能夠觸達目標受眾。 2. 培訓內部團隊：企業應考慮培訓內部的公關或行銷團隊，特別是在具專業性或品牌推廣為主要目標的影片製作上，這樣不僅能降低製作成本，還能確保影片內容的準確性與一致性。 未來研究可以進一步探討不同產業、產品類型下，影片製作策略對於品牌曝光與效益提升的具體影響，為企業提供更加精準的行銷決策依據。 1.5 參考文獻 https://www.ingentaconnect.com/content/hsp/jdsmm/2015/00000003/00000001/art00004 https://eloncdn.blob.core.windows.net/eu3/sites/153/2017/06/08West.pdf "],["text-mining-of-youtube-comments.html", "2 Text Mining of Youtube Comments 2.1 準備 R + Python 環境 2.2 建置 Dataset 2.3 文字探勘分析", " 2 Text Mining of Youtube Comments 2.1 準備 R + Python 環境 #準備Anaconda（虛擬環境管理平台，解決pip安裝包衝突） #準備ChromeDriver （藉由Py自動化控制Google Chrome） library(reticulate) #用來整合Py的R套件 #設定Python環境 use_python(&quot;/opt/anaconda3/envs/emajor/bin/python&quot;, required = T) use_condaenv(&quot;emajor&quot;) # 安裝pandas模組 # py_install(&quot;pandas&quot;) # （ Selenium &amp; webdriver_manager 要用終端機conda安裝 ） #開啟Ptyhon腳本，用Py的套件進行爬蟲 2.2 建置 Dataset 2.2.1 By API 尋找影片標籤 from googleapiclient.discovery import build api_key = &#39;YOUR_API_KEY&#39; youtube = build(&#39;youtube&#39;, &#39;v3&#39;, developerKey=api_key) def get_video_categories(region_code=&#39;TW&#39;): categories_response = youtube.videoCategories().list( part=&#39;snippet&#39;, regionCode=region_code ).execute() categories = [] for item in categories_response[&#39;items&#39;]: categories.append({ &#39;id&#39;: item[&#39;id&#39;], &#39;title&#39;: item[&#39;snippet&#39;][&#39;title&#39;] }) return categories categories = get_video_categories() for category in categories: print(f&quot;Category ID: {category[&#39;id&#39;]}, Title: {category[&#39;title&#39;]}&quot;) # 讀取 CSV 文件 file_path &lt;- &quot;/Users/liam/Desktop/NTNU/碩二/1131/文字探勘/Category.csv&quot; data &lt;- read.csv(file_path) # 顯示前 5 行的具體列 library(knitr) kable(data[, c(&quot;ID&quot;, &quot;Category&quot;)], caption = &quot;YouTube 音樂類影片前五列數據&quot;) 2.2.2 By API 查找符合條件的 url 清單 import random import pandas as pd from googleapiclient.discovery import build api_key = &#39;YOUR_API_KEY&#39; youtube = build(&#39;youtube&#39;, &#39;v3&#39;, developerKey=api_key) keyword = &#39;Music&#39; def search_videos(max_results=50): search_response = youtube.search().list( part=&#39;snippet&#39;, type=&#39;video&#39;, q=keyword, maxResults=max_results, order=&#39;viewCount&#39; ).execute() video_ids = [item[&#39;id&#39;][&#39;videoId&#39;] for item in search_response[&#39;items&#39;]] video_details = youtube.videos().list( id=&#39;,&#39;.join(video_ids), part=&#39;snippet,statistics&#39; ).execute() videos_over_100m = [] for item in video_details[&#39;items&#39;]: view_count = int(item[&#39;statistics&#39;][&#39;viewCount&#39;]) if view_count &gt;= 10000000: videos_over_100m.append({ &#39;id&#39;: item[&#39;id&#39;], &#39;title&#39;: item[&#39;snippet&#39;][&#39;title&#39;], &#39;url&#39;: f&quot;https://www.youtube.com/watch?v={item[&#39;id&#39;]}&quot;, &#39;views&#39;: view_count }) return videos_over_100m all_videos = [] attempts = 0 while len(all_videos) &lt; 200 and attempts &lt; 5: videos = search_videos(max_results=50) new_videos = [video for video in videos if video[&#39;id&#39;] not in [v[&#39;id&#39;] for v in all_videos]] if new_videos: all_videos.extend(new_videos) attempts = 0 else: attempts += 1 df = pd.DataFrame(all_videos) df.to_csv(&#39;/Users/liam/Desktop/NTNU/碩二/1131/文字探勘/youtube_music_videos.csv&#39;, index=False) # 讀取 CSV 文件 file_path &lt;- &quot;/Users/liam/Desktop/NTNU/碩二/1131/文字探勘/youtube_Music2_videos_over_100m.csv&quot; data &lt;- read.csv(file_path) # 顯示前 5 行的具體列 library(knitr) kable(head(data[, c(&quot;title&quot;, &quot;url&quot;,&quot;views&quot;)]), caption = &quot;YouTube 音樂類影片前五列數據&quot;) 2.2.3 爬取留言 import pandas as pd from selenium import webdriver from selenium.webdriver.chrome.service import Service from selenium.webdriver.chrome.options import Options from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC import time video_data = pd.read_csv(&#39;/Users/liam/Desktop/NTNU/碩二/1131/文字探勘/youtube_music_videos.csv&#39;) options = Options() options.add_argument(&quot;--disable-blink-features=AutomationControlled&quot;) options.add_argument(&quot;--disable-infobars&quot;) options.add_argument(&quot;--disable-extensions&quot;) options.add_argument(&quot;start-maximized&quot;) options.add_experimental_option(&quot;excludeSwitches&quot;, [&quot;enable-automation&quot;]) options.add_experimental_option(&#39;useAutomationExtension&#39;, False) service = Service(executable_path=&quot;/Users/liam/Desktop/NTNU/碩二/1131/文字探勘/chromedriver-mac-arm64/chromedriver&quot;) driver = webdriver.Chrome(service=service, options=options) def scroll_to_load_comments(): last_height = driver.execute_script(&#39;return document.documentElement.scrollHeight&#39;) while True: driver.execute_script(&quot;window.scrollBy(0, 800);&quot;) time.sleep(2) comments = driver.find_elements(By.XPATH, &#39;//*[@id=&quot;content-text&quot;]&#39;) if len(comments) &gt;= 30: break new_height = driver.execute_script(&#39;return document.documentElement.scrollHeight&#39;) if new_height == last_height: break last_height = new_height def get_comments(video_url): driver.get(video_url) try: scroll_to_load_comments() WebDriverWait(driver, 10).until( EC.presence_of_element_located((By.XPATH, &#39;//*[@id=&quot;content-text&quot;]&#39;)) ) time.sleep(2) comments = driver.find_elements(By.XPATH, &#39;//*[@id=&quot;content-text&quot;]&#39;)[:30] return [comment.text for comment in comments] except Exception as e: print(f&quot;留言區未加載: {e}&quot;) return [] all_comments = [] for index, row in video_data.iterrows(): video_url = row[&#39;url&#39;] title = row[&#39;title&#39;] views = row[&#39;views&#39;] try: comments = get_comments(video_url) for comment in comments: all_comments.append({ &#39;title&#39;: title, &#39;url&#39;: video_url, &#39;views&#39;: views, &#39;comment&#39;: comment }) except Exception as e: print(f&quot;無法爬取 {video_url}: {e}&quot;) df = pd.DataFrame(all_comments) df.to_csv(&#39;/Users/liam/Desktop/NTNU/碩二/1131/文字探勘/youtube_comments.csv&#39;, index=False) driver.quit() # 讀取 CSV 文件 file_path &lt;- &quot;/Users/liam/Desktop/NTNU/碩二/1131/文字探勘/youtube_comments_music.csv&quot; data &lt;- read.csv(file_path) # 顯示前 5 行的具體列 library(knitr) kable(head(data[, c(&quot;title&quot;, &quot;comment&quot;)]), caption = &quot;YouTube 音樂類影片前五列數據&quot;) 2.3 文字探勘分析 2.3.1 套件準備 install.packages(&quot;tm&quot;) install.packages(&quot;SnowballC&quot;) install.packages(&quot;jiebaR&quot;) install.packages(&quot;tidytext&quot;) install.packages(&quot;dplyr&quot;) install.packages(&quot;ggplot2&quot;) install.packages(&quot;wordcloud&quot;) install.packages(&quot;syuzhet&quot;) install.packages(&quot;igraph&quot;) install.packages(&quot;topicmodels&quot;) install.packages(&quot;forcats&quot;) install.packages(&quot;textstem&quot;) install.packages(&quot;udpipe&quot;) 2.3.2 文本清理 library(tidytext) library(dplyr) library(ggplot2) # 讀取 CSV 文件 file_path &lt;- &quot;/Users/liam/Desktop/NTNU/碩二/1131/文字探勘/youtube_comments_music.csv&quot; comments_df &lt;- read.csv(file_path) # 假設你的評論列名是 &#39;comment&#39; comments &lt;- comments_df$comment # 將評論轉換為資料框 comments_df &lt;- data.frame(line = 1:length(comments), text = comments) # 清理文本 comments_df$text &lt;- tolower(comments_df$text) # 轉換為小寫 comments_df$text &lt;- gsub(&quot;[[:punct:]]&quot;, &quot; &quot;, comments_df$text) # 移除標點符號 comments_df$text &lt;- gsub(&quot;[[:digit:]]&quot;, &quot; &quot;, comments_df$text) # 移除數字 comments_df$text &lt;- gsub(&quot;\\\\s+&quot;, &quot; &quot;, comments_df$text) # 移除多餘空白 # 將評論拆分為單詞 wordfile &lt;- unnest_tokens(comments_df, word, text, token = &quot;words&quot;) # 加載標準停用詞列表 stop_words &lt;- data.frame(tidytext::stop_words) # 移除停用詞 wordfile &lt;- anti_join(wordfile, stop_words) # 擴展停用詞列表，加入縮寫詞和口語詞 custom_stop_words &lt;- data.frame(word = c(&quot;hai&quot;,&quot;bhai&quot;,&quot;don&quot;, &quot;ve&quot;, &quot;ll&quot;, &quot;na&quot;, &quot;el&quot;, &quot;la&quot;, &quot;️️️&quot;, &quot;’ll&quot;, &quot;‘till &quot;, &quot;’ve&quot;, &quot;‘littl&quot;, &quot;’re&quot;, &quot;“’m&quot;, &quot;“’re&quot;, &quot;“bon&quot;, &quot;“copi&quot;, &quot;“don’t&quot;,&quot;you&quot;, &quot;the&quot;, &quot;and&quot;, &quot;you&quot;, &quot;that&quot;, &quot;for&quot;)) # 移除自定義的停用詞 wordfile &lt;- anti_join(wordfile, custom_stop_words) ##進階處理 library(forcats) library(SnowballC) # 使用詞幹提取來處理複數詞(怪怪的) #wordfile$word &lt;- wordStem(wordfile$word, language = &quot;en&quot;) # 加載 textstem 套件（better） library(textstem) # 使用 lemmatize_words 進行詞形還原 wordfile$word &lt;- lemmatize_words(wordfile$word) # 重新計算詞頻 wordfreq &lt;- count(wordfile, word, sort = TRUE) # 查看前 20 個最常見的詞 wordfreqdf20 &lt;- wordfreq[1:20,] # 打印結果 wordfreqdf20 # 重新排列詞語順序以便進行可視化 wordfreqdf20$word &lt;- fct_reorder(wordfreqdf20$word, wordfreqdf20$n, .desc = TRUE) 2.3.3 Case 1 詞頻&amp;文字雲 # 繪製詞頻圖 ggplot(data = wordfreqdf20, aes(x = word, y = n, fill = word)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;, width = 0.5) + coord_flip() + labs(title = &quot;YouTube Comments Freq&quot;, x = &quot;word&quot;, y = &quot;freq&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) library(wordcloud) # 構建詞與詞頻數據，假設你的詞頻表已經計算好了 wordfreq &lt;- count(wordfile, word, sort = TRUE) # 繪製文字雲 wordcloud(words = wordfreq$word, freq = wordfreq$n, min.freq = 4, # 設定最低出現頻率 random.order = FALSE, # 詞語按照頻率大小排序 colors = brewer.pal(8, &quot;Dark2&quot;)) # 設定顏色 library(wordcloud2) # 使用 wordcloud2 繪製互動式文字雲 wordcloud2(data = wordfreq, size = 1, color = &#39;random-light&#39;, backgroundColor = &quot;black&quot;) 2.3.4 Case 2 情感分析 # 加載必要的套件 library(tidytext) library(dplyr) # 使用 &quot;nrc&quot; 情感詞典進行情感分析 wordfile_sentiments &lt;- inner_join(wordfile, get_sentiments(&quot;nrc&quot;), by = &quot;word&quot;) # 允許多對多關係 wordfile_sentiments &lt;- inner_join(wordfile, get_sentiments(&quot;nrc&quot;), by = &quot;word&quot;, relationship = &quot;many-to-many&quot;) # 統計各種情感的出現頻率 sentiment_counts &lt;- wordfile_sentiments %&gt;% count(sentiment, sort = TRUE) # 繪製情感條形圖 ggplot(sentiment_counts, aes(x = sentiment, y = n, fill = sentiment)) + geom_bar(stat = &quot;identity&quot;) + theme_minimal() + ggtitle(&quot;YouTube Comments Sentiment Analysis&quot;) 2.3.5 Case 3 主題模型 # 加載 topicmodels 套件進行主題模型分析 library(topicmodels) library(tm) # 構建文檔-詞矩陣 (DTM) dtm_clean &lt;- DocumentTermMatrix(Corpus(VectorSource(wordfile$word))) # 檢查 DTM 中的空行數量 row_sums &lt;- rowSums(as.matrix(dtm_clean)) dtm_clean &lt;- dtm_clean[row_sums &gt; 0, ] # 去除沒有詞語的行 # 設置主題數量 (例如 5 個主題) lda_model &lt;- LDA(dtm_clean, k = 3, control = list(seed = 1234)) # 顯示每個主題中的關鍵詞 topics &lt;- terms(lda_model, 10) print(topics) # 可視化每個主題中的前幾個關鍵詞 topic_terms &lt;- as.data.frame(topics) print(topic_terms) # 提取每個評論的主題分佈 topic_distribution &lt;- tidy(lda_model, matrix = &quot;gamma&quot;) # 可視化評論中的主題分佈 ggplot(topic_distribution, aes(x = topic, y = gamma, fill = as.factor(topic))) + geom_bar(stat = &quot;identity&quot;) + theme_minimal() + labs(title = &quot;Topic Distribution in Comments&quot;, x = &quot;Topic&quot;, y = &quot;Proportion&quot;) 2.3.6 Case 4 詞性標注 # 加載 udpipe 套件 library(udpipe) # 下載預訓練的英文模型 ud_model &lt;- udpipe_download_model(language = &quot;english&quot;) # 加載模型 ud_model &lt;- udpipe_load_model(ud_model$file_model) # 使用模型進行詞性標註 pos_result &lt;- udpipe_annotate(ud_model, x = comments_df$text) # 轉換為數據框格式 pos_df &lt;- as.data.frame(pos_result) # 顯示詞性標註的結果 head(pos_df, 10) "],["next-step.html", "3 Next Step?", " 3 Next Step? 評論趨勢分析 • 評論量隨時間變化的趨勢： • 你可以分析影片在發布後的評論量隨時間的變化。例如，評論數量是否在某些時段爆發？這樣可以幫助了解影片在什麼時候受到最多的關注。 • 可視化評論量隨時間的趨勢曲線，這可以幫助判斷影片受關注的高峰期和持續多久。 • 影片評論生命周期： • 分析評論的生命周期，即影片發布後多長時間內的評論是最多的，評論活動如何隨時間減少。這有助於理解影片的「熱度」維持了多久。 • 評論的發表頻率： • 你可以計算單位時間（如每小時、每天）的評論量，從而分析評論的發表頻率，並探索某些時間段是否評論更為活躍。 評論情感隨時間的變化 • 情感分析趨勢： • 結合情感分析，你可以研究影片在不同行為階段的情感變化。例如，影片發布後最初幾天的評論可能以正面居多，但隨著時間的推移，可能會出現更多負面評論。你可以將評論時間戳記與情感分析結合起來，分析正負面情感隨時間的變化。 • 分析「高峰期」內的情感趨勢，看看是否在某些時期情感波動特別大，這可能與特定的外部事件或影響因素相關。 評論的時間分佈分析 • 評論的日夜分佈： • 你可以分析評論主要集中在哪些時間段發表，例如白天、晚上，甚至具體的時區。這可以幫助你理解觀眾來自哪些地區，或者影片的受眾是否集中在某些時段更活躍。 • 評論的時區分析： • 如果你有影片觀眾的地域分佈數據（如透過 YouTube Analytics），可以將評論時間戳與時區對應，從而分析不同地區觀眾的評論習慣。例如，影片可能在某些國家的夜間突然獲得大量評論，這可以揭示潛在的國際市場。 事件驅動的評論分析 • 特定事件或話題驅動的評論波動： • 如果影片內容涉及某些時事話題或事件，你可以檢查事件發生前後評論數量和內容的變化。例如，在影片發布後，某個新聞事件可能引發了更多評論，這樣你可以了解評論是否受到特定事件驅動。 • 促銷或網紅效應： • 如果影片經過某些推廣活動（如網紅合作或廣告推送），你可以通過時間戳分析該活動對評論數量和情感的影響，判斷促銷活動或網紅推薦的效果。 評論的熱點分析 • 評論高峰時間： • 分析評論發佈的集中時間段，確定評論熱點。例如，評論是否集中於影片發布後的幾小時或幾天內，這可以幫助企業選擇最佳的推廣時間，或者調整推廣策略。 • 評論的回應性分析： • 分析在一條熱門評論發佈後，後續評論的回應性。例如，是否有評論驅動了後續大量的互動或爭論。你可以找出一些「引爆點」的評論，這些評論可能引發了更多關注和互動。 評論者行為分析 • 重複評論者行為分析： • 如果可以將時間戳記與評論者信息結合起來，你可以分析同一評論者在不同行為階段的參與情況。例如，某些評論者可能會在影片發布初期給出正面評論，隨著時間推移再度發表評論。 • 評論者活躍時間分析： • 你可以分析評論者發表評論的活躍時間，看看是否有某些評論者經常在影片發布的特定時段評論。 回應和互動分析 • 影片發布後評論的互動性： • 你可以分析影片發布後多久開始有更多的互動（回應和點讚），這可以幫助了解觀眾參與的時效性。 • 透過時間戳，還可以計算評論之間的回應間隔，分析影片下的社群互動情況。 大規模的資料可以對母體樣本進行分析，並幫助建立有意義的預測模型。 時間戳記提供評論量的時間趨勢，還可以揭示影片的生命周期、情感波動、受眾行為等多個面向的變量。 通過結合影片的特徵和觀眾行為模式，這些數據能夠為影片的推廣策略提供有價值的預測，進而指導企業或上傳者做出更好的影片創作與發布決策。 "],["重點回顧.html", "4 11/22 重點回顧 4.1 Step.1 刪除非英文評論 4.2 Step.2 文本再清理 4.3 Step.3 變數修正（時間&amp;按讚數） 4.4 What now..最新資料視圖 4.5 轉換寬格式的必要？ 4.6 Step.4 ML model selection", " 4 11/22 重點回顧 4.1 Step.1 刪除非英文評論 # 讀取資料 library(dplyr) data &lt;- read.csv(&quot;/Users/liam/Desktop/NTNU/碩二/1131/文字探勘/youtube_comments_with_likes_1114.csv&quot;) # 使用正則表達式檢測僅包含英語字母、數字和標點的評論 data_clean &lt;- data %&gt;% mutate(comment = as.character(comment)) %&gt;% filter(grepl(&quot;^[A-Za-z0-9\\\\s.,!?\\\\&#39;\\&quot;-]+$&quot;, comment)) # 保存清理後的結果 #write.csv(data_clean, &quot;cleaned_youtube_comments.csv&quot;, row.names = FALSE) cat(&quot;清理完成，剩餘&quot;, nrow(data_clean), &quot;條英語系評論。\\n&quot;) #install.packages(&quot;textcat&quot;) # 讀取資料並檢測語言 library(textcat) data &lt;- read.csv(&quot;/Users/liam/Desktop/NTNU/碩二/1131/文字探勘/youtube_comments_with_likes_1114.csv&quot;) # 檢測評論語言 data_clean &lt;- data %&gt;% mutate(comment = as.character(comment), language = textcat(comment)) %&gt;% filter(language == &quot;english&quot;) # 過濾僅包含英語的評論 # 保存清理後的結果 #write.csv(data_clean, &quot;cleaned_youtube_comments_t2.csv&quot;, row.names = FALSE) cat(&quot;清理完成，剩餘&quot;, nrow(data_clean), &quot;條英語系評論。\\n&quot;) 4.2 Step.2 文本再清理 ## 加載套件 library(tm) ## Loading required package: NLP library(SnowballC) library(tidytext) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(ggplot2) ## ## Attaching package: &#39;ggplot2&#39; ## The following object is masked from &#39;package:NLP&#39;: ## ## annotate library(textstem) ## Loading required package: koRpus.lang.en ## Loading required package: koRpus ## Loading required package: sylly ## For information on available language packages for &#39;koRpus&#39;, run ## ## available.koRpus.lang() ## ## and see ?install.koRpus.lang() ## ## Attaching package: &#39;koRpus&#39; ## The following object is masked from &#39;package:tm&#39;: ## ## readTagged # 讀取 CSV 文件 file_path &lt;- &quot;/Users/liam/Desktop/NTNU/碩二/1131/文字探勘/cleaned_youtube_comments_t2.csv&quot; comments_df &lt;- read.csv(file_path) # 假設你的 CSV 包含 &#39;comment&#39; 和 &#39;views&#39; 列 # 增加 line 編號作為每條評論的唯一標識 comments_df &lt;- comments_df %&gt;% mutate(line = row_number()) # 清理文本 comments_df$text &lt;- tolower(comments_df$comment) # 轉換為小寫 comments_df$text &lt;- gsub(&quot;[[:punct:]]&quot;, &quot; &quot;, comments_df$text) # 移除標點符號 comments_df$text &lt;- gsub(&quot;[[:digit:]]&quot;, &quot; &quot;, comments_df$text) # 移除數字 comments_df$text &lt;- gsub(&quot;\\\\s+&quot;, &quot; &quot;, comments_df$text) # 移除多餘空白 # 將評論拆分為單詞並保留 line 和 views 信息 wordfile &lt;- comments_df %&gt;% unnest_tokens(word, text, token = &quot;words&quot;) %&gt;% dplyr::select(line, word, views, timestamp, likes) # 加載標準停用詞列表 stop_words &lt;- tidytext::stop_words # 移除停用詞 wordfile &lt;- wordfile %&gt;% anti_join(stop_words, by = &quot;word&quot;) # 擴展停用詞列表，加入縮寫詞和口語詞 custom_stop_words &lt;- data.frame(word = c(&quot;hai&quot;,&quot;bhai&quot;,&quot;don&quot;,&quot;music&quot;,&quot;listen&quot;, &quot;song&quot;,&quot;ina&quot;,&quot;video&quot;,&quot;watch&quot;,&quot;day&quot;, &quot;edit&quot;, &quot;ve&quot;, &quot;ll&quot;, &quot;na&quot;, &quot;el&quot;, &quot;la&quot;, &quot;️️️&quot;, &quot;’ll&quot;, &quot;‘till &quot;, &quot;’ve&quot;, &quot;‘littl&quot;, &quot;’re&quot;, &quot;“’m&quot;, &quot;“’re&quot;, &quot;“bon&quot;, &quot;“copi&quot;, &quot;“don’t&quot;,&quot;you&quot;, &quot;the&quot;, &quot;and&quot;, &quot;you&quot;, &quot;that&quot;, &quot;for&quot;, &quot;doesn&quot;,&quot;didn&quot;,&quot;facemoji&quot;,&quot;bio&quot;, &quot;interesrting&quot;,&quot;awesomeeee&quot;,&quot;asome&quot;, &quot;hardd&quot;,&quot;kocham&quot;,&quot;১ম&quot;,&quot;২০&quot;,&quot;৪০&quot;,&quot;á&quot;, &quot;aa&quot;,&quot;aaaaaaaaaaahhhhhhhhhhhh&quot;, &quot;aaaaaaaaaaa&quot;,&quot;aaaaaaaaaa&quot;,&quot;aaaaaaaa&quot;, &quot;aaa&quot;,&quot;aaaaaah&quot;,&quot;aaaaaand&quot;,&quot;aæaæaæaæaæa&quot;, &quot;aaj&quot;,&quot;aaliyah&quot;,&quot;aam&quot;,&quot;aani&quot;,&quot;ab&quot;)) # 移除自定義的停用詞 wordfile &lt;- wordfile %&gt;% anti_join(custom_stop_words, by = &quot;word&quot;) # 使用 lemmatize_words 進行詞形還原 wordfile$word &lt;- lemmatize_words(wordfile$word) # 移除數字 wordfile &lt;- wordfile %&gt;% filter(!grepl(&quot;\\\\d+&quot;, word)) # 重新計算詞頻 wordfreq &lt;- count(wordfile, word, sort = TRUE) # 查看前 20 個最常見的詞 wordfreqdf20 &lt;- wordfreq[1:20,] # 打印結果 wordfreqdf20 ## word n ## 1 love 1672 ## 2 people 1080 ## 3 time 852 ## 4 vote 587 ## 5 bro 549 ## 6 life 511 ## 7 guy 465 ## 8 feel 438 ## 9 trump 431 ## 10 country 356 ## 11 play 351 ## 12 watch 339 ## 13 world 317 ## 14 hear 316 ## 15 real 316 ## 16 game 305 ## 17 call 299 ## 18 live 296 ## 19 comment 290 ## 20 voice 286 # 計算 TF-IDF wordfile_with_tfidf &lt;- wordfile %&gt;% count(line, word, views, timestamp, likes) %&gt;% # 計算每行每個單詞的頻率 bind_tf_idf(word, line, n) # 計算 TF-IDF # 查看結果 head(wordfile_with_tfidf) ## line word views timestamp likes n tf idf tf_idf ## 1 1 amaze 41022107 5 個月前 4904 1 0.125 4.029920 0.5037400 ## 2 1 code 41022107 5 個月前 4904 1 0.125 7.074442 0.8843053 ## 3 1 comment 41022107 5 個月前 4904 1 0.125 4.533446 0.5666808 ## 4 1 link 41022107 5 個月前 4904 1 0.125 7.559950 0.9449938 ## 5 1 receive 41022107 5 個月前 4904 1 0.125 6.931342 0.8664177 ## 6 1 reward 41022107 5 個月前 4904 1 0.125 7.847632 0.9809540 4.2.1 Case 1 詞頻&amp;文字雲 # 繪製詞頻圖 ggplot(data = wordfreqdf20, aes(x = word, y = n, fill = word)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;, width = 0.5) + coord_flip() + labs(title = &quot;YouTube Comments Freq&quot;, x = &quot;word&quot;, y = &quot;freq&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) library(wordcloud) ## Loading required package: RColorBrewer # 構建詞與詞頻數據，假設你的詞頻表已經計算好了 wordfreq &lt;- count(wordfile, word, sort = TRUE) # 繪製文字雲 wordcloud(words = wordfreq$word, freq = wordfreq$n, min.freq = 200, # 設定最低出現頻率 random.order = FALSE, # 詞語按照頻率大小排序 colors = brewer.pal(8, &quot;Dark2&quot;)) # 設定顏色 library(wordcloud2) # 使用 wordcloud2 繪製互動式文字雲 wordcloud2(data = wordfreq, size = 1, color = &#39;random-light&#39;, backgroundColor = &quot;black&quot;) 4.2.2 Case 2 情感分析 # 加載必要的套件 library(tidytext) library(dplyr) # 使用 &quot;nrc&quot; 情感詞典進行情感分析 wordfile_sentiments &lt;- inner_join(wordfile, get_sentiments(&quot;nrc&quot;), by = &quot;word&quot;) ## Warning in inner_join(wordfile, get_sentiments(&quot;nrc&quot;), by = &quot;word&quot;): Detected an unexpected many-to-many ## relationship between `x` and `y`. ## ℹ Row 3 of `x` matches multiple ## rows in `y`. ## ℹ Row 4195 of `y` matches multiple ## rows in `x`. ## ℹ If a many-to-many relationship is ## expected, set `relationship = ## &quot;many-to-many&quot;` to silence this ## warning. # 允許多對多關係 wordfile_sentiments &lt;- inner_join(wordfile, get_sentiments(&quot;nrc&quot;), by = &quot;word&quot;, relationship = &quot;many-to-many&quot;) # 統計各種情感的出現頻率 sentiment_counts &lt;- wordfile_sentiments %&gt;% count(sentiment, sort = TRUE) # 繪製情感條形圖 ggplot(sentiment_counts, aes(x = sentiment, y = n, fill = sentiment)) + geom_bar(stat = &quot;identity&quot;) + theme_minimal() + ggtitle(&quot;YouTube Comments Sentiment Analysis&quot;) 4.2.3 Case 3 主題模型 library(topicmodels) library(tidytext) library(dplyr) library(ggplot2) library(tidyr) # 假設數據存儲在 `wordfile_with_tfidf` data &lt;- wordfile_with_tfidf # 過濾無意義的單詞，確保詞頻不為 0 filtered_data &lt;- data %&gt;% filter(!is.na(word) &amp; !is.na(tf_idf) &amp; tf_idf &gt; 0) # 創建詞頻矩陣 (DocumentTermMatrix) dtm &lt;- filtered_data %&gt;% count(line, word, wt = tf_idf) %&gt;% # 使用 tf_idf 作為詞頻權重 cast_dtm(line, word, n) # 轉換為 DocumentTermMatrix 格式 # 設置主題數量 (可調整) k &lt;- 5 filtered_data &lt;- filtered_data %&gt;% mutate(tf_idf_int = round(tf_idf * 100)) # 將 tf_idf 轉換為非負整數 dtm &lt;- filtered_data %&gt;% count(line, word, wt = tf_idf_int) %&gt;% # 使用非負整數權重 cast_dtm(line, word, n) # 轉換為 DocumentTermMatrix 格式 # 訓練 LDA 模型 lda_model &lt;- LDA(dtm, k = k, control = list(seed = 1234)) # 查看模型摘要 lda_model ## A LDA_VEM topic model with 5 topics. # 提取每個主題的關鍵詞 (beta 矩陣) lda_topics &lt;- tidy(lda_model, matrix = &quot;beta&quot;) # 查看每個主題的前 10 個關鍵詞 lda_top_terms &lt;- lda_topics %&gt;% group_by(topic) %&gt;% slice_max(beta, n = 10) %&gt;% ungroup() # 可視化每個主題的關鍵詞 lda_top_terms %&gt;% mutate(term = reorder_within(term, beta, topic)) %&gt;% ggplot(aes(beta, term, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;) + scale_y_reordered() + labs(title = &quot;LDA Topics Analysis&quot;, x = &quot;Beta&quot;, y = &quot;word&quot;) # 提取每個文檔的主題分佈 (gamma 矩陣) lda_documents &lt;- tidy(lda_model, matrix = &quot;gamma&quot;) # 查看第 6 個文檔的主題分佈 lda_documents %&gt;% filter(document == &quot;6&quot;) %&gt;% arrange(desc(gamma)) ## # A tibble: 5 × 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 6 1 0.538 ## 2 6 3 0.354 ## 3 6 2 0.108 ## 4 6 5 0.000109 ## 5 6 4 0.000109 # 定義要測試的主題數量 topics &lt;- c(2, 5, 10, 15, 20) # 訓練不同主題數量的 LDA 模型 ldas &lt;- lapply(topics, function(k) { LDA(dtm, k = k, control = list(seed = 1234)) }) # 計算困惑度 perplexities &lt;- sapply(ldas, perplexity) # 可視化困惑度與主題數量的關係 data.frame(k = topics, perplexity = perplexities) %&gt;% ggplot(aes(k, perplexity)) + geom_line() + geom_point() + labs(title = &quot;Num x Perplex&quot;, x = &quot;Topic Num&quot;, y = &quot;perplexity&quot;) 4.3 Step.3 變數修正（時間&amp;按讚數） # 創建清理 timestamp 的函數 clean_timestamp &lt;- function(timestamp) { # 移除 &quot;(已編輯)&quot; 的內容 timestamp &lt;- gsub(&quot;\\\\(已編輯\\\\)&quot;, &quot;&quot;, timestamp) # 處理不同的時間單位，並轉換為天數 timestamp_days &lt;- ifelse( grepl(&quot;年前&quot;, timestamp), as.numeric(gsub(&quot;年前&quot;, &quot;&quot;, timestamp)) * 365, # 年轉換為天 ifelse( grepl(&quot;個月前&quot;, timestamp), as.numeric(gsub(&quot;個月前&quot;, &quot;&quot;, timestamp)) * 30, # 月轉換為天 ifelse( grepl(&quot;週前&quot;, timestamp), as.numeric(gsub(&quot;週前&quot;, &quot;&quot;, timestamp)) * 7, # 週轉換為天 ifelse( grepl(&quot;天前&quot;, timestamp), as.numeric(gsub(&quot;天前&quot;, &quot;&quot;, timestamp)), # 天保持不變 0 # 無法識別的時間，預設為 0 ) ) ) ) return(timestamp_days) } library(dplyr) # 將 timestamp 轉換為統一的天數 wordfile_with_tfidf &lt;- wordfile_with_tfidf %&gt;% mutate( timestamp_days = clean_timestamp(timestamp) # 新增統一格式的天數欄位 ) library(dplyr) # 檢查哪些值轉換後是 NA problematic_likes &lt;- wordfile_with_tfidf %&gt;% filter(is.na(as.numeric(gsub(&quot;萬&quot;, &quot;&quot;, likes)))) %&gt;% select(likes) %&gt;% distinct() wordfile_with_tfidf &lt;- wordfile_with_tfidf %&gt;% mutate(likes = case_when( grepl(&quot;^\\\\d+(\\\\.\\\\d+)?萬$&quot;, likes) ~ as.numeric(sub(&quot;萬&quot;, &quot;&quot;, likes)) * 10000, # 處理「X萬」格式 grepl(&quot;^\\\\d+$&quot;, likes) ~ as.numeric(likes), # 處理純數字 TRUE ~ NA_real_ # 其他情況設為 NA )) # 檢查轉換後的結果 head(wordfile_with_tfidf$likes) # 假設 wordfile_with_tfidf 已經存在於 R 環境中 #output_path &lt;- &quot;/Users/liam/Desktop/NTNU/碩二/1131/文字探勘/wordfile_with_tfidf.csv&quot; # 將資料框寫入 CSV 檔案 #write.csv(wordfile_with_tfidf, file = output_path, row.names = FALSE) # 確認輸出 #cat(&quot;檔案已輸出至：&quot;, output_path) 4.4 What now..最新資料視圖 # 讀取 CSV 文件 file_path &lt;- &quot;/Users/liam/Desktop/NTNU/碩二/1131/文字探勘/wordfile_with_tfidf.csv&quot; data &lt;- read.csv(file_path) # 顯示前 5 行的具體列 library(knitr) kable(head(data[, c(&quot;line&quot;, &quot;word&quot;, &quot;views&quot;, &quot;timestamp&quot;, &quot;likes&quot;, &quot;n&quot;, &quot;tf&quot;, &quot;idf&quot;, &quot;tf_idf&quot;, &quot;timestamp_days&quot;)]), caption = &quot;Data 現在長這樣&quot;) (#tab:show raw data)Data 現在長這樣 line word views timestamp likes n tf idf tf_idf timestamp_days 1 amaze 41022107 5 個月前 4904 1 0.125 4.029920 0.5037400 150 1 code 41022107 5 個月前 4904 1 0.125 7.074442 0.8843053 150 1 comment 41022107 5 個月前 4904 1 0.125 4.533446 0.5666808 150 1 link 41022107 5 個月前 4904 1 0.125 7.559950 0.9449938 150 1 receive 41022107 5 個月前 4904 1 0.125 6.931342 0.8664177 150 1 reward 41022107 5 個月前 4904 1 0.125 7.847632 0.9809540 150 4.5 轉換寬格式的必要？ # 載入套件 library(dplyr) library(tidyr) # 讀取資料 data &lt;- read.csv(&quot;/Users/liam/Desktop/NTNU/碩二/1131/文字探勘/wordfile_with_tfidf.csv&quot;, stringsAsFactors = FALSE) data &lt;- data %&gt;% rename(line_id = line) # 資料預處理 data$line_id &lt;- as.factor(data$line_id) data$word &lt;- as.character(data$word) # 將資料轉換為寬格式（詞彙矩陣） tf_idf_matrix &lt;- data %&gt;% select(line_id, word, tf_idf) %&gt;% pivot_wider( names_from = word, values_from = tf_idf, values_fill = list(tf_idf = 0) ) # 提取每則留言的其他特徵 other_features &lt;- data %&gt;% group_by(line_id) %&gt;% summarize( views = first(views), likes = first(likes), timestamp = first(timestamp), timestamp_days = first(timestamp_days) ) # 合併詞彙矩陣與其他特徵 final_data &lt;- tf_idf_matrix %&gt;% left_join(other_features, by = &quot;line_id&quot;) # 處理缺失值 final_data$likes[is.na(final_data$likes)] &lt;- 0 # 設置觀看次數的上限 upper_limit &lt;- 10000000 # 1000 萬 # 對觀看次數做截斷 final_data &lt;- final_data %&gt;% mutate(views_capped = ifelse(views &gt; upper_limit, upper_limit, views)) # 過濾 final_data_filtered &lt;- final_data %&gt;% filter(views &lt;= upper_limit) head(final_data) 4.6 Step.4 ML model selection 老師建議： 1.布瓦松 2.負二項 3.Y分類-&gt;成功、不成功、待成功 123 ####################################################### ## 長格式資料、不tune ####################################################### # 載入必要的套件 library(tidyverse) library(tidymodels) library(textrecipes) # 假設您的資料已讀取，並命名為 data # 資料包含以下欄位：line_id, word, views, timestamp, likes, n, tf, idf, tf_idf, timestamp_days # 定義觀看次數的區間和對應的分類 recode_views &lt;- function(views) { if (views &gt;= 500000 &amp; views &lt; 1000000) { return(1) } else if (views &gt;= 1000000 &amp; views &lt; 2000000) { return(2) } else if (views &gt;= 2000000 &amp; views &lt; 4000000) { return(3) } else if (views &gt;= 4000000 &amp; views &lt; 5000000) { return(4) } else if (views &gt;= 5000000 &amp; views &lt; 6000000) { return(5) } else if (views &gt;= 7000000 &amp; views &lt; 8000000) { return(6) } else if (views &gt;= 8000000 &amp; views &lt; 9000000) { return(7) } else if (views &gt;= 9000000 &amp; views &lt; 10000000) { return(8) } else if (views &gt;= 10000000 &amp; views &lt; 15000000) { return(9) } else if (views &gt;= 15000000) { return(10) } else { return(NA) } } # 應用編碼函數並移除缺失值 data &lt;- data %&gt;% mutate(views_category = sapply(views, recode_views)) %&gt;% drop_na(views_category) %&gt;% mutate(views_category = factor(views_category)) # 將目標變數轉換為因子 # 處理數值型特徵，將 likes 和 timestamp_days 轉換為數值型，並處理缺失值 data &lt;- data %&gt;% mutate( likes = as.numeric(likes), likes = ifelse(is.na(likes), 0, likes), timestamp_days = as.numeric(timestamp_days), timestamp_days = ifelse(is.na(timestamp_days), 0, timestamp_days) ) # 分割資料（確保每個 line_id 只出現在訓練或測試集之一） set.seed(1234) line_ids &lt;- unique(data$line_id) train_ids &lt;- sample(line_ids, size = 0.7 * length(line_ids)) train_data &lt;- data %&gt;% filter(line_id %in% train_ids) test_data &lt;- data %&gt;% filter(!line_id %in% train_ids) train_data &lt;- train_data %&gt;% dplyr::select(-c(tf, idf, timestamp, views)) ml_recipe &lt;- recipe(views_category ~ word , data = train_data) %&gt;% step_dummy(all_nominal_predictors(), -all_outcomes()) %&gt;% step_normalize(all_numeric_predictors()) ml_recipe &lt;- recipe(views_category ~ word + tf_idf , data = train_data) %&gt;% # 移除不必要的欄位（若有） step_other(word, threshold = 0.01) %&gt;% # 將出現頻率低於1%的詞彙歸類為 &quot;other&quot; step_dummy(word) %&gt;% # 將 &#39;word&#39; 轉換為虛擬變數 step_normalize(all_numeric_predictors()) # 標準化數值型預測變數 # 檢查配方是否正確 prep(ml_recipe, training = train_data) %&gt;% juice() %&gt;% head() # 建立模型規格（未調參） lasso_spec &lt;- multinom_reg(penalty = 0.1, mixture = 1) %&gt;% # LASSO 邏輯回歸 set_engine(&quot;glmnet&quot;) svm_spec &lt;- svm_linear(cost = 1) %&gt;% # 線性 SVM set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;LiblineaR&quot;) null_spec &lt;- null_model() %&gt;% # Null 模型 set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;parsnip&quot;) # 建立工作流程 lasso_workflow &lt;- workflow() %&gt;% add_recipe(ml_recipe) %&gt;% add_model(lasso_spec) svm_workflow &lt;- workflow() %&gt;% add_recipe(ml_recipe) %&gt;% add_model(svm_spec) null_workflow &lt;- workflow() %&gt;% add_recipe(ml_recipe) %&gt;% add_model(null_spec) # 訓練模型 final_lasso_fit &lt;- fit(lasso_workflow, data = train_data) final_svm_fit &lt;- fit(svm_workflow, data = train_data) final_null_fit &lt;- fit(null_workflow, data = train_data) # 預測與評估 # 定義評估指標 eval_metrics &lt;- metric_set(accuracy, kap, roc_auc) # LASSO 模型評估 lasso_predictions &lt;- predict(final_lasso_fit, test_data, type = &quot;prob&quot;) %&gt;% bind_cols(predict(final_lasso_fit, test_data)) %&gt;% bind_cols(test_data %&gt;% select(views_category)) lasso_metrics &lt;- lasso_predictions %&gt;% eval_metrics(truth = views_category, estimate = .pred_class) print(&quot;LASSO 模型評估結果：&quot;) print(lasso_metrics) # SVM 模型評估 svm_predictions &lt;- predict(final_svm_fit, test_data, type = &quot;prob&quot;) %&gt;% bind_cols(predict(final_svm_fit, test_data)) %&gt;% bind_cols(test_data %&gt;% select(views_category)) svm_metrics &lt;- svm_predictions %&gt;% eval_metrics(truth = views_category, estimate = .pred_class) print(&quot;SVM 模型評估結果：&quot;) print(svm_metrics) # Null 模型評估 null_predictions &lt;- predict(final_null_fit, test_data) %&gt;% bind_cols(test_data %&gt;% select(views_category)) null_metrics &lt;- null_predictions %&gt;% eval_metrics(truth = views_category, estimate = .pred_class) print(&quot;Null 模型評估結果：&quot;) print(null_metrics) # 合併所有模型的評估結果 all_metrics &lt;- bind_rows( lasso_metrics %&gt;% mutate(model = &quot;LASSO&quot;), svm_metrics %&gt;% mutate(model = &quot;SVM&quot;), null_metrics %&gt;% mutate(model = &quot;Null Model&quot;) ) print(&quot;所有模型的評估結果：&quot;) print(all_metrics) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
